{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Modeling\n",
    "\n",
    "Text Modeling - Allows us to efficiently analyze large volumes of text by clustering documents into topics. \n",
    "\n",
    "A large amount of text data is unlabelled meaning we won't be able to apply our previous supervised learning approaches to create machine learning models for the data. If we have unlabelled data, we can attempt to discover labels. In case of text data, this means attempting to discover clusters of documents, grouped together by topic.\n",
    "\n",
    "It is difficult to evaluate an unsupervised learning model's effectiveness because we did not actually knew the correct topic or right answer to begin with. All we know is that the documents clustered together share some sort of similar topic ideas. There is not really a good way to evaluate how well did unsupervised learning algorithms do since we never had a right answer to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latend Dirichlet Allocation:\n",
    "\n",
    "Johann Peter Gustav Lejeune Dirichlet was a german mathematician in 1800's who contributed widely to the field of modern mathematics. There is a probability distribution named after him called \"Dirichlet Distribution\".\n",
    "\n",
    "Latent Dirichlet Allocation is based off this probability distribution. In 2003, LDA was first published as a graphical model for topic discovery.\n",
    "\n",
    "Assumptions of LDA for Topic Modeling:\n",
    "1. Documents with similar topics use similar groups of words.\n",
    "2. Latent topics can then be found by searching for groups that frequently occur together in documents across the corpus.\n",
    "\n",
    "![documents with similar topics use similar group of words](https://i.imgur.com/nkYuxjz.jpg)\n",
    "![Latent topics over words](https://i.imgur.com/owCLwoQ.jpg)\n",
    "\n",
    "\n",
    "Documents are probability distributions over latent topics. Topics themselves are probability distributions over words.\n",
    "\n",
    "LDA represents documents as mixtures of topics that spit out words with certain probabilities. \n",
    "\n",
    "It assumes that documents are produced in the following fashion:\n",
    "\n",
    "1. Decide on the number of words N the document will have.\n",
    "2. Choose a topic mixture for the document ( over a fixed set of K topics)\n",
    "3. First picking a topic according to the multinomial distribution that we sampled previously.\n",
    "4. Using the topic to generate the word itself.\n",
    "\n",
    "Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.\n",
    "\n",
    "We choose some fixed number of K topics to discover from a set of documents, and we want to use LDA to learn the topic representation of each document and the words associated to each topic. Then we go through each document and randomly assign each word in the document to one of K topics. This random assignment already gives both topic representation of all the documents and word distributions of all the topics( The initial random topics won't make sense). The we iterate over every word in every document to improve these topics. At the end we have each document assigned to a topic. We can also search for the words that have the highest probability of being assigned to a topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
