{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key classification metrics we need to understand are:\n",
    "\n",
    "    1. Accuracy\n",
    "    2. Recall\n",
    "    3. Precision\n",
    "    4. F1 Score\n",
    "\n",
    "Typically in any classification task, the model can achieve only 2 results:\n",
    "\n",
    "    1. Either the model will be correct in its own prediction\n",
    "    2. Or model was incorrect in it's own prediction\n",
    "\n",
    "Accuracy in classification problems is the number of correct predictions made by the model divided by the total number of predictions. Accuracy is really useful when the target class are well balanced. However, accuracy is not a good choice with unbalanced classes. In order to better get to know about which is the best measure to be used, we need to understand recall and precision. \n",
    "\n",
    "F1 score - A combination of precision and recall.\n",
    "\n",
    "Recall - Ability of the Machine learning model to find all the relevant cases within a dataset. Precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives.\n",
    "\n",
    "Precision - Ability of a classification model to identify only the relevant data points. Precision is defined as the number of true positives divided by the number of true positives plus the number of false positives.\n",
    "\n",
    "We often have a trade-off between recall and precision. While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant that actually were relevant. We can combine both to get F1 score if we are looking for an optimal blend of precision and recall.\n",
    "\n",
    "F1 score is the harmonic mean of the precision and recall taking both metrics into account in the following equation :\n",
    "\n",
    "$$F1 score = 2 * [precision*recall] / [precision+recall]$$\n",
    "\n",
    "A harmonic mean punishes extreme values so that's why we prefer taking h.m. over simple average. Example - A classifier with a precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1 score of 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Metrics\n",
    "\n",
    "In a classification problem, during the testing phase we will have 2 categories :\n",
    "\n",
    "1. True Condition\n",
    "2. Predicted Condition\n",
    "\n",
    "![Confusion Matrix](https://imgur.com/JA3wPiO.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
