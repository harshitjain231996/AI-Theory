{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common issue faced by RNN's is that the network tend to forget their first inputs as information is lost with each forward step going through the RNN. We need some sort of \"long-term memory\" for our network. We need to balance both the short term as well as long term memory cells in the RNN network. During back propagation, RNN's suffer from vanishing gradient problem. Gradient is the value we use to update the value of neural networks weight. The vanishing gradient is when it shrinks when it backpropagates through time. In RNN's the layer that do not get high value of gradient do not learn much which is usually the earlier layers and hence are said to have short term memory.\n",
    "\n",
    "![lstm](https://i.imgur.com/cR879nY.jpg)\n",
    " \n",
    "The LSTM(Long Short-Term Memory) cell was created to help address these RNN issues. They have internal mechanisms calld gates which help with the flow of information. These gates can know which information is important to be kept and which one is to be discarded. Therefore the network can learn and use relevant information in order to make predictions.\n",
    "\n",
    "First words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one. While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.\n",
    "\n",
    "First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network.\n",
    "\n",
    "An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM’s cells.\n",
    "\n",
    "The core concept of LSTM’s are the cell state, and it’s various gates. The cell state act as a transport highway that transfers relative information all the way down the sequence chain. You can think of it as the “memory” of the network. The cell state, in theory, can carry relevant information throughout the processing of the sequence. So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get’s added or removed to the cell state via gates. The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training.\n",
    "\n",
    "***Forget gate*** decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.\n",
    "\n",
    "\n",
    "***Input Gate***: To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output. \n",
    "\n",
    "\n",
    "***Cell State***: To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.\n",
    "\n",
    "\n",
    "***output Gate***: Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.\n",
    "\n",
    "\n",
    "First step in LSTM is called forget gates layer. This step will decide what information we are going to forget or throw away from the cell state. In the first step, we pass in h(t-1) and x(t) after we perform a linear transformation with some weights and biased terms into a sigmoid function. Since the sigmoid layer is going to output a number between 0 and 1, where 1 represents keep the data and 0 represents to forget about it. Example - If we think of a langauge model where we are trying to predict the very next words based on previous ones, a cell state might include the gender of the present subject so that one ends up picking the correct pronoun. When we talk about a new subject, we might want to forget about the gender of the old subject. \n",
    "\n",
    "![forget gate layer](https://i.imgur.com/pbZv9pT.jpg)\n",
    "\n",
    "The next step is about what new information we are going to store in the cell state. The first part is a sigmoid layer and the next part is a hyperbolic tangent layer. Sigmoid layer is called input gate layer i.e. IFT for input gate layer. We take h(t-1) and x(t) in order to do a linear transformation on it with W(i) and b(i), we pass it into a sigmoid function, and due to this we have a bunch of values between 0 and 1's. The hyperbolic tangent layer is going to take h(t-1) and x(t). It will do linear transformation and pass it through a hyperbolic tangent which ends up creating a vector of is commonly known as new candidate values. These candidate values could be added to the state.\n",
    "\n",
    "![2nd step](https://i.imgur.com/jqYx74I.jpg)\n",
    "\n",
    "Now it's time to update the old cell state. The old cell state is c(t-1) and we eventually want to update to the new cell state c(t) so that we can pass that on to the t+1 state of the cell.\n",
    "In the first step we decided what are we going to forget and in the next step we decided what are we going to store. Now we just need to execute those actions. We multiply the old state c(t-1) by f(t). We eventually end up forgetting the things we decided we are going to forget. Then we are going to add it with i(t) multiplied by candidate value. These are the new candidate values and they are being scaled by how much we update these scaled values.  \n",
    "\n",
    "![3rd step](https://i.imgur.com/ImvFY0Y.jpg)\n",
    "\n",
    "Our final decision is going to be what do we output for h(t). This output is based off cell state which is just a filtered division. We are taking h(t-1) and x(t), doing a linear transformation and passing it through the sigmoid and then once we have that output, we are going to multiply by the hyperbolic tangent of c(t).\n",
    "\n",
    "![final state](https://i.imgur.com/MbqAaGW.jpg)\n",
    "\n",
    "Another version of LSTM is called GRU or Gated Recurrent Unit. It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state. It is simpler than LSTM.\n",
    "\n",
    "![GRU](https://i.imgur.com/ujcH5KX.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
